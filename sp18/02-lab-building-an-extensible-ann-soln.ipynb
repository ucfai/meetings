{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Extensible Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring, [TensorFlow](https://tensorflow.org/)\n",
    "\n",
    "We'll be classifying the MNIST dataset.\n",
    "\n",
    "MNIST is ~70,000 images of handwritten digits &ndash; bear in mind, this is essentially a solved problem, so we're not doing anything novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**What you should leave with:** You should leave here with a practical understanding of how to implement an Extensible Artificial Neural Network (ANN) in TensorFlow, from scratch. The concepts don't change when you move to different domains, simply the way in which you apply them. You understanding of the _central_ concept of ANNs, **backpropagation (backprop)** should be well-founded and given some more practice, you ought to be able to explain this to a friend.\n",
    "\n",
    "You should also leave here with an itch to scratch about TensorFlow, and how using such a library can speed up your model development, as well as understanding some of it's drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "\n",
    "1. [Some Prep-processing](#1-Some-Pre-processing)\n",
    "2. [Building an ANN from \"Scratch\"](#2-Building-an-ANN-from-\"Scratch\")\n",
    "3. [Peeking Inside the Network](#3-Peeking-Inside-the-Network)\n",
    "4. [Exploring TensorBoard](#4-Exploring-TensorBoard)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Some Pre-processing (&amp; Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T21:25:11.717696Z",
     "start_time": "2018-02-15T21:25:06.568770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## globals\n",
    "LOG_DIR = \"/tmp/mnist/\"\n",
    "DAT_DIR = \"/tmp/mnist/data/\"\n",
    "\n",
    "## learning rate\n",
    "LRN_RAT = 0.5\n",
    "\n",
    "## loading MNIST and setting up looping meta\n",
    "mnist = input_data.read_data_sets(DAT_DIR, one_hot=True)\n",
    "_epochs = 6\n",
    "BATCHES = 100\n",
    "NXAMPLS = (_epochs * mnist.train.images.shape[0]) // BATCHES\n",
    "\n",
    "## making sure the LOG_DIR is empty - we'll need this for TensorBoard\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building an ANN from \"Scratch\"\n",
    "\n",
    "We say \"Scratch\" because TensorFlow actually does quite a bit of head lifting for us. We'll still walk through how you might code this up in raw [Python](https://python.org/), [NumPy](http://numpy.org), and [SciPy](http://scipy.org), at the end.\n",
    "\n",
    "Something worth noting is that building the ANN yourself, **_can_** run faster than the ANN we'll be building with TensorFlow; but libraries like [TensorFlow](https://tensorflow.org), [PyTorch](http://pytorch.org), [Keras](https://keras.io), and the like speed up your construction of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last semeseter, we built a class &ndash; tonight, we won't. This is largely to do with the way in which TensorFlow works. The `scope`-ing we'll be doing tonight will be similar to the encapsulation classes allow for. We'll take a look at the TensorBoard on occasion; so before we move on...\n",
    "\n",
    "**Let's open up a terminal window from Jupyter**\n",
    "1. Open `localhost:19972`\n",
    "2. Click on \"New\" and pick \"Terminal\"\n",
    "3. `tensorboard --logdir=/tmp/mnist`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's dive into some code.**\n",
    "\n",
    "First, we need to create a TensorFlow `InteractiveSession`. This will let our code seem a little more normal.\n",
    "\n",
    "TensorFlow is quirky in the way you write code, but a lot of this will make sense once we get through to the end. Bear with me. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.741524Z",
     "start_time": "2018-02-15T20:57:50.861337Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Setup the IO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... what do we want our network to do?\n",
    "\n",
    "1. We want to take in 28x28 pixel images\n",
    "2. We want to classify them into digits from 0..9\n",
    "\n",
    "It's worth noting that $28^2 = 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.745687Z",
     "start_time": "2018-02-15T20:57:51.742592Z"
    }
   },
   "outputs": [],
   "source": [
    "INP_DIM = 784\n",
    "OUT_DIM = 10\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    x  = tf.placeholder(tf.float32, [None, INP_DIM], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUT_DIM], name=\"y-input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-up! What are `tf.name_scope`s? Good question. Depending on your programming style, you may use your functions to segment portions of your program/classes into \"logical\" groupings. `tf.name_scope`s are how we do this with TensorFlow. Now, we could just do standard programming, but `tf.name_scope`s allow us to use the TensorBoard to visualize what's going on, and better understand the structure of our Neural Network.\n",
    "\n",
    "---\n",
    "\n",
    "Also... what's a `tf.placeholder`? Again, good question. `tf.placeholders` are basically promises to TensorFlow that you'll supply the information to fill these variables. You'll see, later, that we use a rather specific way to feed `tf.placeholder`s data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Image Previews to TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at this in the TensorBoard &ndash; we can use the \"Images\" tab to preview what our images in the training and testing sets look like for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.768397Z",
     "start_time": "2018-02-15T20:57:51.746644Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input_reshape\"):\n",
    "    img_shape_inp = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image(\"input\", img_shape_inp, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our code a little more descriptive and somewhat portable, let's export `Bias` and `Weight` creation to functions that we can call by specifying the shape instead of having to hand-write these, every, single, time. (Commas for emphasis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.782118Z",
     "start_time": "2018-02-15T20:57:51.773216Z"
    }
   },
   "outputs": [],
   "source": [
    "def var_biases(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.800572Z",
     "start_time": "2018-02-15T20:57:51.788417Z"
    }
   },
   "outputs": [],
   "source": [
    "def var_weight(shape):\n",
    "    init = tf.truncated_normal(shape, stddev=pow(shape[0], -0.5))\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for us to be able to export certain charts and histograms to the TensorBoard. If you'd like, I can elaborate on what's happening here in more detail &ndash; if so, I'll do my best to update these notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.826627Z",
     "start_time": "2018-02-15T20:57:51.806468Z"
    }
   },
   "outputs": [],
   "source": [
    "def var_summarize(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        \n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        \n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        \n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        \n",
    "        tf.summary.histogram(\"histogram\", var)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why write lines upon lines of code, when you can write a function?! :D\n",
    "\n",
    "This function builds each one of our layers, with consistent `tf.name_scope`s and attaches the necessary TensorBoard utilities for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.848362Z",
     "start_time": "2018-02-15T20:57:51.828926Z"
    }
   },
   "outputs": [],
   "source": [
    "def Layer(name, inp_ten, inp_dim, out_dim, act=tf.nn.sigmoid):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        with tf.name_scope(\"weights\"):\n",
    "            weights = var_weight([inp_dim, out_dim])\n",
    "            var_summarize(weights)\n",
    "            \n",
    "#         with tf.name_scope(\"biases\"):\n",
    "#             biases  = var_biases([out_dim])\n",
    "#             var_summarize(biases)\n",
    "            \n",
    "        with tf.name_scope(\"Wx_plus_b\"):\n",
    "            pre_activation = tf.matmul(inp_ten, weights)# + biases\n",
    "            tf.summary.histogram(\"pre_activation\", pre_activation)\n",
    "            \n",
    "        activation = act(pre_activation, name=\"activation\")\n",
    "        tf.summary.histogram(\"activations\", activation)\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Network Overhead's Done, Let's Build it Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nodes` is how we'll specify the number of nodes in a given layer.\n",
    "- `layers` is a list that stores references to all the inputs of the network.\n",
    "\n",
    "We'll also derive some other information to assign the layer while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.924611Z",
     "start_time": "2018-02-15T20:57:51.860273Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def arch_check(nodes):\n",
    "    return len(nodes) >= 2\n",
    "\n",
    "nodes = [INP_DIM, 200, 200, 200, OUT_DIM]\n",
    "\n",
    "layers = [x]\n",
    "\n",
    "if arch_check(nodes):\n",
    "    for idx in range(len(nodes) - 2):\n",
    "        name = \"hidden_{}\".format(idx)\n",
    "        prev = layers[idx]\n",
    "        inp_dim = nodes[idx]\n",
    "        out_dim = nodes[idx + 1]\n",
    "        new_lyr = Layer(name, prev, inp_dim, out_dim)\n",
    "        layers.append(new_lyr)\n",
    "\n",
    "    # Do not apply softmax activation yet, see below.\n",
    "    y = Layer(\"output\", layers[-1], nodes[-2], nodes[-1], act=tf.identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Cost Function\n",
    "\n",
    "Like we talked about, briefly, in the last lecture... we need a cost function &ndash; otherwise our network can't converge. \n",
    "\n",
    "We'll be using cross-entropy loss (or log loss). It's defined at: $H_{y'}(y) = -\\Sigma_i y'_i log(y_i)$. $y$ is our predicted probabilities and $y'$ is the true probabilities. You can imagine this as cross-entropy measuring how inefficient our predictions are at describing the truth. [If you want to learn more.](https://colah.github.io/posts/2015-09-Visual-Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.944870Z",
     "start_time": "2018-02-15T20:57:51.925747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'xentropy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    \n",
    "    with tf.name_scope(\"total\"):\n",
    "        xentropy = tf.reduce_mean(diff)\n",
    "        \n",
    "tf.summary.scalar(\"xentropy\", xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the structure of TensorFlow, we'll be using the build-in gradient descent optimizer. (If you want to look at more of TensorFlow's Optimizers, you'll find them [here](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/training/).)\n",
    "\n",
    "We should recall that **Gradient Descent** is strictly an optimization algorithm; recently it's become less popular in favor of newer algorithms like [Adam](https://arxiv.org/abs/1412.6980) and [RMSprop](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf#page=26) which offer some advantages over Gradient Descent.\n",
    "\n",
    "Let's review how Gradient Descent works, though; before moving past this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.967558Z",
     "start_time": "2018-02-15T20:57:51.945881Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LRN_RAT).minimize(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to tell TensorFlow how we want to validate our predictions and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:51.976582Z",
     "start_time": "2018-02-15T20:57:51.968598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    with tf.name_scope(\"correct_prediction\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some TensorBoard Overhead\n",
    "\n",
    "This is simply code to export our logs for the TensorBoard to read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T23:55:46.629211Z",
     "start_time": "2018-02-15T23:55:46.597999Z"
    }
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "trn_writer = tf.summary.FileWriter(LOG_DIR + \"train/\", sess.graph)\n",
    "tst_writer = tf.summary.FileWriter(LOG_DIR + \"test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### On the Home Stretch to Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that `feed_dict()` does for us is construct our mini-batches to make the looping a tad prettier.\n",
    "\n",
    "This is how we'll feed those `tf.placeholder`s we wrote up ages ago. TensorFlow uses the `feed_dict` parameter in `sess.run()` to assign values to `tf.placeholder`s in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:57:52.158228Z",
     "start_time": "2018-02-15T20:57:52.154920Z"
    }
   },
   "outputs": [],
   "source": [
    "def feed_dict(train):\n",
    "    xs, ys = mnist.train.next_batch(BATCHES) if train else (mnist.test.images, mnist.test.labels)\n",
    "    \n",
    "    return {x: xs, y_: ys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network on MNIST\n",
    "\n",
    "Now, let's move on to training the network! We've already specified our parameters, now we just need to tell TensorFlow to do its thing. :D\n",
    "\n",
    "**Tangent:** `tf.global_variables_initializer().run()` is how the TensorFlow Graph actually starts up. Up until now we're really just been writing mark-up. `tf...run()` allows us to actually act on the Compute Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T20:58:46.204043Z",
     "start_time": "2018-02-15T20:57:52.159079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for step in range(NXAMPLS):\n",
    "    if step % 10 == 0:\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "        tst_writer.add_summary(summary, step)\n",
    "    else:\n",
    "        if step % 100 == 99:\n",
    "            run_opts = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_meta = tf.RunMetadata()\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True), \n",
    "                                  options=run_opts, run_metadata=run_meta)\n",
    "            trn_writer.add_run_metadata(run_meta, \"step{:03d}\".format(step))\n",
    "            trn_writer.add_summary(summary, step)\n",
    "        else:\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "            trn_writer.add_summary(summary, step)\n",
    "        \n",
    "trn_writer.close()\n",
    "tst_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until we look into Backquery, we'll be hangin' out in the TensorBoard, it'll be open at `localhost:19973`. Let's hop over there for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to anticipated time-constraints, I'll post this notebook with Backquery implemented, but the workshop won't have it."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "github": "ionlights",
    "name": "John Muchovej"
   }
  ],
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "footer": "<footer id=\"slide_foot\">\n  <div  id=\"slide_foot-brand\">\n    <span class=\"ucfsigai-brand\"></span>\n  </div>\n  <div  id=\"slide_foot-unit\">\n    <span class=\"text-gold\"> U1: </span>&nbsp;<span class=\"text-white\"> Neural Networks </span>\n  </div>\n  <a    id=\"slide_foot-attend\" href=\"https://goo.gl/\">\n      <span class=\"text-white\"> https://goo.gl/ </span>\n  </a>\n  <div  id=\"slide_foot-date\">\n    <span class=\"text-white\"> Feb 08, 2018 </span>\n  </div>\n</footer>\n",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
