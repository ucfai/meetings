{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, SVG, IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Neural Nets SVG dims, this is for the NN forward-pass walkthrough.\n",
    "# SVG's default size is 732x518 ~ which is the (width * 0.7076502732)\n",
    "NN_SVG_W = 732\n",
    "NN_SVG_H = math.ceil(NN_SVG_W * 0.7076502732) + 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Extensible Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring, [TensorFlow](https://tensorflow.org/)\n",
    "\n",
    "We'll be classifying the MNIST dataset.\n",
    "\n",
    "MNIST is ~70,000 images of handwritten digits &ndash; bear in mind, this is essentially a solved problem, so we're not doing anything novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**What you should leave with:** You should leave here with a practical understanding of how to implement an Extensible Artificial Neural Network (ANN) in TensorFlow, from scratch. The concepts don't change when you move to different domains, simply the way in which you apply them. You understanding of the _central_ concept of ANNs, **backpropagation (backprop)** should be well-founded and given some more practice, you ought to be able to explain this to a friend.\n",
    "\n",
    "You should also leave here with an itch to scratch about TensorFlow, and how using such a library can speed up your model development, as well as understanding some of it's drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "\n",
    "1. [The Math Behind Backprop](#1.-The-Math-Behind-Backprop)\n",
    "2. [Some Prep-processing](#2.-Some-Pre-processing-(&-Setup))\n",
    "3. [Building an ANN from \"Scratch\"](#3.-Building-an-ANN-from-\"Scratch\")\n",
    "4. [Exploring TensorBoard](#4.-Exploring-the-TensorBoard)\n",
    "5. [Peeking Inside the Network](#5.-Peeking-Inside-the-Network) (Time permitting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. The Math Behind Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Math Primer](https://github.com/ucfsigai/meetings/sp18/math-primer1.ipynb), we talked about derivatives, partials, vectors, dot and cross products, and matrices.\n",
    "\n",
    "There's one thing we're missing before we can actually build a Neural Network though.\n",
    "\n",
    "Let's think back to the Neural Network lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "IFrame(\"assets/nns/fwd.html\", width=NN_SVG_W, height=NN_SVG_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "So, let's introduce some notation:\n",
    "\n",
    "Let's subscript the `x` layer (`x1`, `x2`, `x3`) and let's do the same for the hidden layers (`h11`, `h12`, ..., `h23`, `h24`).\n",
    "\n",
    "Now let's try calculating the inputs to `h11`. It would be...\n",
    "$h_{11} = x_1 \\cdot w_{11} + x_2 \\cdot w_{21} + x_3 \\cdot w_{31}$, right?\n",
    "\n",
    "That works, but now let's do the inputs to `h12`...\n",
    "$h_{12} = x_1 \\cdot w_{12} + x_2 \\cdot w_{22} + x_3 \\cdot w_{32}$.\n",
    "\n",
    "And for `h13`...\n",
    "\n",
    "$h_{13} = x_1 \\cdot w_{13} + x_2 \\cdot w_{23} + x_3 \\cdot w_{33}$.\n",
    "\n",
    "Finally, for `h14`...\n",
    "\n",
    "$h_{14} = x_1 \\cdot w_{14} + x_2 \\cdot w_{24} + x_3 \\cdot w_{34}$, right?\n",
    "\n",
    "Does anyone see a pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Let's drag this out, so it's aligned a little more nicely:\n",
    "\n",
    "$h_{11} = x_1 \\cdot w_{11} + x_2 \\cdot w_{21} + x_3 \\cdot w_{31}$\n",
    "\n",
    "$h_{12} = x_1 \\cdot w_{12} + x_2 \\cdot w_{22} + x_3 \\cdot w_{32}$\n",
    "\n",
    "$h_{13} = x_1 \\cdot w_{13} + x_2 \\cdot w_{23} + x_3 \\cdot w_{33}$\n",
    "\n",
    "$h_{14} = x_1 \\cdot w_{14} + x_2 \\cdot w_{24} + x_3 \\cdot w_{34}$\n",
    "\n",
    "Does this look like a certain math operation we learned 2 weeks ago?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yes! It looks an awful lot like a dot product.**\n",
    "\n",
    "So, let's try to represent this like a dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "% x_{11}\n",
    "\\begin{bmatrix}\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, an observation that may not be immediately obvious is that we can represent sets of vectors as a matrix. This isn't terribly important for **_us_**, because calculating a dot-product on between a $4 \\times 3$ Matrix and a $3$-row Vector is time consuming, especially if we have to do it 100K times.\n",
    "\n",
    "But, there's a computational reason for this, and I won't cover it now &ndash; but doing a dot product saves us countless for loops and doesn't affect our application of activation functions and backprop! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Some Pre-processing (&amp; Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## globals\n",
    "LOG_DIR = \"/tmp/mnist/\"\n",
    "DAT_DIR = \"/tmp/mnist/data/\"\n",
    "\n",
    "## learning rate\n",
    "LRN_RAT = 0.5\n",
    "\n",
    "## loading MNIST and setting up looping meta\n",
    "mnist = input_data.read_data_sets(DAT_DIR, one_hot=True)\n",
    "_epochs = 6\n",
    "BATCHES = 100\n",
    "NXAMPLS = (_epochs * mnist.train.num_examples) // BATCHES\n",
    "\n",
    "## making sure the LOG_DIR is empty - we'll need this for TensorBoard\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Building an ANN from \"Scratch\"\n",
    "\n",
    "We say \"Scratch\" because TensorFlow actually does quite a bit of head lifting for us. We'll still walk through how you might code this up in raw [Python](https://python.org/), [NumPy](http://numpy.org), and [SciPy](http://scipy.org), at the end.\n",
    "\n",
    "Something worth noting is that building the ANN yourself, **_can_** run faster than the ANN we'll be building with TensorFlow; but libraries like [TensorFlow](https://tensorflow.org), [PyTorch](http://pytorch.org), [Keras](https://keras.io), and the like speed up your construction of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last semeseter, we built a class &ndash; tonight, we won't. This is largely to do with the way in which TensorFlow works. The `scope`-ing we'll be doing tonight will be similar to the encapsulation classes allow for. We'll take a look at the TensorBoard on occasion; so before we move on...\n",
    "\n",
    "**Let's open up a terminal window from Jupyter**\n",
    "1. Open `localhost:19972`\n",
    "2. Click on \"New\" and pick \"Terminal\"\n",
    "3. `tensorboard --logdir=/tmp/mnist`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's dive into some code.**\n",
    "\n",
    "First, we need to create a TensorFlow `InteractiveSession`. This will let our code seem a little more normal.\n",
    "\n",
    "TensorFlow is quirky in the way you write code, but a lot of this will make sense once we get through to the end. Bear with me. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Setup the IO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... what do we want our network to do?\n",
    "\n",
    "1. We want to take in 28x28 pixel images\n",
    "2. We want to classify them into digits from 0..9\n",
    "\n",
    "It's worth noting that $28^2 = 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIM = ## FILL THIS IN\n",
    "OUT_DIM = ## FILL THIS IN\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    x  = tf.placeholder(tf.float32, [None, INP_DIM], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUT_DIM], name=\"y-input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-up! What are `tf.name_scope`s? Good question. Depending on your programming style, you may use your functions to segment portions of your program/classes into \"logical\" groupings. `tf.name_scope`s are how we do this with TensorFlow. Now, we could just do standard programming, but `tf.name_scope`s allow us to use the TensorBoard to visualize what's going on, and better understand the structure of our Neural Network.\n",
    "\n",
    "---\n",
    "\n",
    "Also... what's a `tf.placeholder`? Again, good question. `tf.placeholders` are basically promises to TensorFlow that you'll supply the information to fill these variables. You'll see, later, that we use a rather specific way to feed `tf.placeholder`s data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Image Previews to TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at this in the TensorBoard &ndash; we can use the \"Images\" tab to preview what our images in the training and testing sets look like for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input_reshape\"):\n",
    "    img_shape_inp = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image(\"input\", img_shape_inp, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our code a little more descriptive and somewhat portable, let's export `Bias` and `Weight` creation to functions that we can call by specifying the shape instead of having to hand-write these, every, single, time. (Commas for emphasis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_biases(shape):\n",
    "    ## FILL THIS IN (we need to initialize the biases, how might we do this?)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_weight(shape):\n",
    "    ## FILL THIS IN (we need to intialize the weights, how might we do this?)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for us to be able to export certain charts and histograms to the TensorBoard. If you'd like, I can elaborate on what's happening here in more detail &ndash; if so, I'll do my best to update these notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_summarize(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        \n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        \n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        \n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        \n",
    "        tf.summary.histogram(\"histogram\", var)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why write lines upon lines of code, when you can write a function?! :D\n",
    "\n",
    "This function builds each one of our layers, with consistent `tf.name_scope`s and attaches the necessary TensorBoard utilities for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layer(name, inp_ten, inp_dim, out_dim, act=tf.nn.sigmoid):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        with tf.name_scope(\"weights\"):\n",
    "            ## FILL THIS IN (this should initialize the weights of a layer)\n",
    "            var_summarize(weights)\n",
    "            \n",
    "        with tf.name_scope(\"biases\"):\n",
    "            ## FILL THIS IN (this should intialize the biases of a layer)\n",
    "            var_summarize(biases)\n",
    "            \n",
    "        with tf.name_scope(\"Wx_plus_b\"):\n",
    "            ## FILL THIS IN (this should perform the dot product and addition prior to activation)\n",
    "            tf.summary.histogram(\"pre_activation\", pre_activation)\n",
    "            \n",
    "        ## FILL THIS IN (we need to pass our data through the activation function)\n",
    "        tf.summary.histogram(\"activations\", activation)\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Network Overhead's Done, Let's Build it Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nodes` is how we'll specify the number of nodes in a given layer.\n",
    "- `layers` is a list that stores references to all the inputs of the network.\n",
    "\n",
    "We'll also derive some other information to assign the layer while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def arch_check(nodes):\n",
    "    return len(nodes) >= 2\n",
    "\n",
    "nodes = [INP_DIM, 200, 200, 200, OUT_DIM]\n",
    "\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "layers = [x]\n",
    "\n",
    "if arch_check(nodes):\n",
    "    for idx in range(len(nodes) - 2):\n",
    "        ## FILL THIS IN (this should generate all but the output layer)\n",
    "\n",
    "    ## FILL THIS IN (this should be the output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Cost Function\n",
    "\n",
    "Like we talked about, briefly, in the last lecture... we need a cost function &ndash; otherwise our network can't converge. \n",
    "\n",
    "We'll be using cross-entropy loss (or log loss). It's defined at: $H_{y'}(y) = -\\Sigma_i y'_i log(y_i)$. $y$ is our predicted probabilities and $y'$ is the true probabilities. You can imagine this as cross-entropy measuring how inefficient our predictions are at describing the truth. [If you want to learn more.](https://colah.github.io/posts/2015-09-Visual-Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    \n",
    "    with tf.name_scope(\"total\"):\n",
    "        xentropy = tf.reduce_mean(diff)\n",
    "        \n",
    "tf.summary.scalar(\"xentropy\", xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the structure of TensorFlow, we'll be using the build-in gradient descent optimizer. (If you want to look at more of TensorFlow's Optimizers, you'll find them [here](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/training/).)\n",
    "\n",
    "We should recall that **Gradient Descent** is strictly an optimization algorithm; recently it's become less popular in favor of newer algorithms like [Adam](https://arxiv.org/abs/1412.6980) and [RMSprop](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf#page=26) which offer some advantages over Gradient Descent.\n",
    "\n",
    "Let's review how Gradient Descent works, though; before moving past this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LRN_RAT).minimize(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to tell TensorFlow how we want to validate our predictions and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    with tf.name_scope(\"correct_prediction\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some TensorBoard Overhead\n",
    "\n",
    "This is simply code to export our logs for the TensorBoard to read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "trn_writer = tf.summary.FileWriter(LOG_DIR + \"train/\", sess.graph)\n",
    "tst_writer = tf.summary.FileWriter(LOG_DIR + \"test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### On the Home Stretch to Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that `feed_dict()` does for us is construct our mini-batches to make the looping a tad prettier.\n",
    "\n",
    "This is how we'll feed those `tf.placeholder`s we wrote up ages ago. TensorFlow uses the `feed_dict` parameter in `sess.run()` to assign values to `tf.placeholder`s in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_dict(train):\n",
    "    xs, ys = mnist.train.next_batch(BATCHES) if train else (mnist.test.images, mnist.test.labels)\n",
    "    \n",
    "    return {x: xs, y_: ys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network on MNIST\n",
    "\n",
    "Now, let's move on to training the network! We've already specified our parameters, now we just need to tell TensorFlow to do its thing. :D\n",
    "\n",
    "**Tangent:** `tf.global_variables_initializer().run()` is how the TensorFlow Graph actually starts up. Up until now we're really just been writing mark-up. `tf...run()` allows us to actually act on the Compute Graph.\n",
    "\n",
    "To execute a training step, we use the `sess.run()` to run through a mini-batch. Mini-batching is a common technique that was initially intended to reduce memory overhead, but was also found to improve the accuracy of most networks. If you're interested in learning more about mini-batching &ndash; speak up, I'll update this afterwards to have links to articles about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for step in range(NXAMPLS):\n",
    "    if step % 10 == 0:\n",
    "        ## get an update of the network's accuracy as we're progressing\n",
    "        ## we're going to use the testing data for this, but we're *_not_* training the network!\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(train=False))\n",
    "        tst_writer.add_summary(summary, step)\n",
    "    else:\n",
    "        ## every 100 steps, we write out some metadata for the TensorBoard to make use of\n",
    "        if step % 100 == 99:\n",
    "            run_opts = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_meta = tf.RunMetadata()\n",
    "            \n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(train=True), \n",
    "                                  options=run_opts, run_metadata=run_meta)\n",
    "            trn_writer.add_run_metadata(run_meta, \"step{:03d}\".format(step))\n",
    "            trn_writer.add_summary(summary, step)\n",
    "        else:\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(train=True))\n",
    "            trn_writer.add_summary(summary, step)\n",
    "        \n",
    "trn_writer.close()\n",
    "tst_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Exploring the TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To watch our network training through TensorBoard, let's open up `localhost:19973`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And... we're done! You've just coded up a Neural Network in TensorFlow and took a walk through TensorBoard."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "github": "ionlights",
    "name": "John Muchovej"
   }
  ],
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "footer": "<footer id=\"slide_foot\">\n  <div  id=\"slide_foot-brand\">\n    <span class=\"ucfsigai-brand\"></span>\n  </div>\n  <div  id=\"slide_foot-unit\">\n    <span class=\"text-gold\"> U1: </span>&nbsp;<span class=\"text-white\"> Neural Networks </span>\n  </div>\n  <a    id=\"slide_foot-attend\" href=\"https://goo.gl/\">\n      <span class=\"text-white\"> https://goo.gl/ </span>\n  </a>\n  <div  id=\"slide_foot-date\">\n    <span class=\"text-white\"> Feb 08, 2018 </span>\n  </div>\n</footer>\n",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
