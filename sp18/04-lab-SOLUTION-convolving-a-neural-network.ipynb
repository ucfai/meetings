{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T03:35:07.067410Z",
     "start_time": "2018-03-08T03:35:07.033536Z"
    }
   },
   "source": [
    "\n",
    "KEY: \n",
    "- <><><><> == I WRITE\n",
    "- ???????? == STUDENT WORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:34:03.116137Z",
     "start_time": "2018-03-08T20:34:03.113190Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, SVG, IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T21:47:33.536856Z",
     "start_time": "2018-03-08T21:47:33.509265Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp70se0ho2.c:2:38: fatal error: /darknet/include/darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include <stdio.h>\n",
    "#include \"/darknet/include/darknet.h\"\n",
    "\n",
    "void main(){printf(\"hello\");}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolving a Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time object detection and classification with YOLO\n",
    "\n",
    "**What you'll learn:** \n",
    "- How a Convolutional Neural Network is implemented in a real world application\n",
    "- How to build convolutional and (max)pooling layers in C\n",
    "- Run YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your terminal (Command prompt)  and **cd** to **[PATH TO THIS REPO]/meetings/darknet/**\n",
    "\n",
    "type \"wget https://pjreddie.com/media/files/yolo.weights\"\n",
    "\n",
    "This will download a pre-trained YOLO into your darknet directory. By the time we run it, the weights should be finished downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learn what you're working with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Darknet is an elegant neural network framework that can build anything from traditional to recurrent to convolutional to long short-term memory (LSTM) neural networks. For this workshop, we will be focusing on running YOLO by rebuilding functions for the convolutional and pooling layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we have to understand the Darknet's architecture. Otherwise, we're just grasping at straws! Below is the abstract structure for a layer. I removed variables that are not in the scope of the workshop to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T21:47:39.887096Z",
     "start_time": "2018-03-08T21:47:39.866385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmpb90jd_pr.c:1:38: fatal error: /darknet/include/darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include \"/darknet/include/darknet.h\"\n",
    "struct layer_ex{\n",
    "    LAYER_TYPE type;\n",
    "    ACTIVATION activation;\n",
    "    COST_TYPE cost_type;\n",
    "    void (*forward)   (struct layer, struct network); //Forward propagation\n",
    "    void (*backward)  (struct layer, struct network); //Backward propagation\n",
    "    void (*update)    (struct layer, update_args);    //updater\n",
    "    void (*forward_gpu)   (struct layer, struct network);\n",
    "    void (*backward_gpu)  (struct layer, struct network);\n",
    "    void (*update_gpu)    (struct layer, update_args);\n",
    "    \n",
    "    int batch_normalize; //boolean -> use batch normalization == 1. There were other normalization methods but we use this\n",
    "    int batch; // # of samples to be fed in\n",
    "    \n",
    "                //vvvv We manipulate these as matrices, but physically, they are 1D arrays w/ calloc\n",
    "    int inputs;    //data fed into the layer\n",
    "    int outputs;   //data pushed out of the layer\n",
    "    int nweights;  // # of weights \n",
    "                //^^^^\n",
    "    int nbiases;    //Additional weights to add\n",
    "\n",
    "    int h,w,c;  //height h and width w of each input 'matrix', and a count c of all the matrices fed in\n",
    "    int out_h, out_w, out_c;  //height h and width w of each output 'matrix', and a count c of all the matrices returned\n",
    "    int n;\n",
    "    \n",
    "    int groups;     //An AlexNet adaption to the convolutional layer. Partitions the kernel (filters)\n",
    "    int size;       //rank of a given input matrix ()\n",
    "    int side;\n",
    "    int stride; //How many columns over the kernel (filter matrix) moves\n",
    "\n",
    "    float alpha;\n",
    "    float beta;\n",
    "    float kappa;\n",
    "\n",
    "    float coord_scale;\n",
    "    float object_scale;\n",
    "    float noobject_scale;\n",
    "    float mask_scale;\n",
    "    float class_scale;\n",
    "    int bias_match;\n",
    "    int random;\n",
    "    float thresh;\n",
    "    int classfix;\n",
    "    int absolute;\n",
    "\n",
    "    int onlyforward;\n",
    "    int stopbackward;\n",
    "    int dontload;\n",
    "    int dontloadscales;\n",
    "\n",
    "    float temperature;\n",
    "    float probability;\n",
    "    float scale;\n",
    "\n",
    "    char  * cweights;\n",
    "    int   * indexes;\n",
    "    int   * input_layers;\n",
    "    int   * input_sizes;\n",
    "    int   * map;\n",
    "    float * rand;\n",
    "    float * cost;\n",
    "    float * state;\n",
    "    float * prev_state;\n",
    "    float * forgot_state;\n",
    "    float * forgot_delta;\n",
    "    float * state_delta;\n",
    "    float * combine_cpu;\n",
    "    float * combine_delta_cpu;\n",
    "\n",
    "    float * concat;\n",
    "    float * concat_delta;\n",
    "\n",
    "    float * binary_weights;\n",
    "\n",
    "    float * biases;\n",
    "    float * bias_updates;\n",
    "\n",
    "    float * scales;\n",
    "    float * scale_updates;\n",
    "\n",
    "    float * weights;\n",
    "    float * weight_updates;\n",
    "\n",
    "    float * delta;\n",
    "    float * output;\n",
    "    float * squared;\n",
    "    float * norms;\n",
    "\n",
    "    float * spatial_mean;\n",
    "    float * mean;\n",
    "    float * variance;\n",
    "\n",
    "    float * mean_delta;\n",
    "    float * variance_delta;\n",
    "\n",
    "    float * rolling_mean;\n",
    "    float * rolling_variance;\n",
    "\n",
    "    float * x;\n",
    "    float * x_norm;\n",
    "\n",
    "    float * m;\n",
    "    float * v;\n",
    "    \n",
    "    float * bias_m;\n",
    "    float * bias_v;\n",
    "    float * scale_m;\n",
    "    float * scale_v;\n",
    "\n",
    "\n",
    "    float *z_cpu;\n",
    "    float *r_cpu;\n",
    "    float *h_cpu;\n",
    "    float * prev_state_cpu;\n",
    "\n",
    "    float *temp_cpu;\n",
    "    float *temp2_cpu;\n",
    "    float *temp3_cpu;\n",
    "\n",
    "    float *dh_cpu;\n",
    "    float *hh_cpu;\n",
    "    float *prev_cell_cpu;\n",
    "    float *cell_cpu;\n",
    "    float *f_cpu;\n",
    "    float *i_cpu;\n",
    "    float *g_cpu;\n",
    "    float *o_cpu;\n",
    "    float *c_cpu;\n",
    "    float *dc_cpu; \n",
    "\n",
    "    float * binary_input;\n",
    "\n",
    "    tree *softmax_tree;\n",
    "\n",
    "    size_t workspace_size;\n",
    "    \n",
    "    /*CUT GPU STUFF FOR EXAMPLES...*/\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Breaking down the ConvNet pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ConvNet is a series of Convolutional and pooling layers appended to a (usually) fully connected neural network (or ANN.) These layers are needed in order to maintain spatial relations between pixels in an image, something that ANNs are unable to do. Our last workshop went over how to program an ANN from scratch. We now want to show how we can add these additional layers to our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/CNN_pipeline.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: For each convolutional layer, we must denote the dimensions of the input. Our inputs will strictly be images, so we have width **w** and height **h** of the input matrix. Since we read the data in as one big array, we partition the array into equal sizes. **c** is the count, as in, how many different matrices we want to be reading in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter (kernel) performs element-wise matrix multiplication in every position that the filter fully \"fits\" within the input matrix. The filter moves column by column and row by row w.r.t its **stride** (e.g. stride of 2 will move the filter two rows/columns at a time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:31:53.918386Z",
     "start_time": "2018-03-08T20:31:53.905074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"assets/cnns/convolution.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f88d8724278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"assets/cnns/convolution.html\", \"1000\", \"500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:06:34.436619Z",
     "start_time": "2018-03-08T20:06:34.408607Z"
    }
   },
   "source": [
    "In Darknet, our filters have an additional property: **groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Convolutional Layers vs. Group Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a traditional convolution looks something like this. Given a set of samples (left), we apply our filters (middle) to create a set of feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/cnns/convlayer_traditional.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **groups**, we essentially partition the filters, which allows the convolutional layer to categorize filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/cnns/convlayer_group.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this into consideration, we will develop a function to create a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:36:30.213161Z",
     "start_time": "2018-03-08T20:36:30.184415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /darknet/src/convolutional_layer.h:4:0,\n",
      "                 from /tmp/tmpdpyh0hbn.c:2:\n",
      "/darknet/src/cuda.h:4:21: fatal error: darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include \"/darknet/include/darknet.h\"\n",
    "#include \"/darknet/src/convolutional_layer.h\"\n",
    "\n",
    "convolutional_layer make_convolutional_layer_ex(int batch, int h, int w, int c, int n, int groups, int size, int stride, int padding, ACTIVATION activation, int batch_normalize, int binary, int xnor, int adam)\n",
    "{\n",
    "    /* WRITE\n",
    "    \n",
    "    INSTANTIATE variables\n",
    "    */\n",
    "    int i;\n",
    "    convolutional_layer l = {0};\n",
    "    l.type = CONVOLUTIONAL;\n",
    "    \n",
    "\n",
    "    l.groups = groups;   //Filter groups. This parameter tells this layer to partition it's weights by this amount\n",
    "    l.h = h;\n",
    "    l.w = w;\n",
    "    l.c = c;\n",
    "    l.n = n;         //prospected number of outputted matrices\n",
    "    l.batch = batch;     //# of samples fed in per run through\n",
    "    l.stride = stride;\n",
    "    l.size = size;\n",
    "    l.pad = padding;\n",
    "    l.batch_normalize = batch_normalize;\n",
    "\n",
    "    l.weights = calloc(c/groups*n*size*size, sizeof(float));\n",
    "    l.weight_updates = calloc(c/groups*n*size*size, sizeof(float));\n",
    "\n",
    "    l.biases = calloc(n, sizeof(float));\n",
    "    l.bias_updates = calloc(n, sizeof(float));\n",
    "\n",
    "    l.nweights = c/groups*n*size*size;\n",
    "    l.nbiases = n;\n",
    "//////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    \n",
    "    /* EXPLAIN\n",
    "    INSTANTIATE WEIGHTS\n",
    "    \n",
    "    \n",
    "    scale = norm of all the input matrices (rank_1^2 + rank_2^2 + ... + rank_(c)^2)^1/2\n",
    "    rand_normal() = Muller transform\n",
    "    size  = rank of each input matrix within the data, but it's set as # rows of matrix\n",
    "    c     = # of matrices\n",
    "    \n",
    "    */\n",
    "\n",
    "    float scale = sqrt(2./(size*size*c/l.groups));\n",
    "    for(i = 0; i < l.nweights; ++i) l.weights[i] = scale*rand_normal();\n",
    "    \n",
    "    //Calculate the dimensions of the output matrix with this ~~~MAGIC~~~ function! \n",
    "    int out_w = convolutional_out_width(l);\n",
    "    int out_h = convolutional_out_height(l);\n",
    "///////////////////////////////////////////\n",
    "    \n",
    "    /* WRITE\n",
    "    \n",
    "    instantiate the data that will be outputted.\n",
    "    So there will be height and width of each output matrix\n",
    "    and the total count of them. Denoted as n now\n",
    "    \n",
    "    */\n",
    "    \n",
    "    \n",
    "    \n",
    "    l.out_h = out_h;\n",
    "    l.out_w = out_w;\n",
    "    l.out_c = n;\n",
    "    l.outputs = l.out_h * l.out_w * l.out_c;\n",
    "    l.inputs = l.w * l.h * l.c;\n",
    "\n",
    "    l.output = calloc(l.batch*l.outputs, sizeof(float));\n",
    "    l.delta  = calloc(l.batch*l.outputs, sizeof(float));\n",
    "\n",
    "    l.forward = forward_convolutional_layer;\n",
    "    l.backward = backward_convolutional_layer;\n",
    "    l.update = update_convolutional_layer;\n",
    "    \n",
    "//////////////////////////////////////////////////\n",
    "    /*EXPLAIN*/\n",
    "    //This is for batch normalization\n",
    "    //Batch Normalization is a method to reduce internal covariate shift in neural networks\n",
    "\n",
    "    l.scales = calloc(n, sizeof(float));\n",
    "    l.scale_updates = calloc(n, sizeof(float));\n",
    "        for(i = 0; i < n; ++i){\n",
    "            l.scales[i] = 1;\n",
    "        }\n",
    "\n",
    "    l.mean = calloc(n, sizeof(float));\n",
    "    l.variance = calloc(n, sizeof(float));\n",
    "\n",
    "    l.mean_delta = calloc(n, sizeof(float));\n",
    "    l.variance_delta = calloc(n, sizeof(float));\n",
    "\n",
    "    l.rolling_mean = calloc(n, sizeof(float));\n",
    "    l.rolling_variance = calloc(n, sizeof(float));\n",
    "    l.x = calloc(l.batch*l.outputs, sizeof(float));\n",
    "    l.x_norm = calloc(l.batch*l.outputs, sizeof(float));\n",
    "   \n",
    "////////////////////////////////////////////////////////\n",
    "    NOT GOING OVER GPU SET UP\n",
    "///////////////////////////////////////////////////////\n",
    "    /*WRITE\n",
    "    \n",
    "    Now all we have to do is set the workspace of this layer with our magic function get_workspace_size(l)\n",
    "    and the activation layer we wish to give it. Almost always will be either LEAKY or RELU\n",
    "    */\n",
    "    l.workspace_size = get_workspace_size(l);\n",
    "    l.activation = activation;\n",
    "\n",
    "    fprintf(stderr, \"conv  %5d %2d x%2d /%2d  %4d x%4d x%4d   ->  %4d x%4d x%4d\\n\", n, size, size, stride, w, h, c, l.out_w, l.out_h, l.out_c);\n",
    "\n",
    "    return l;\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up the function to create convolutional layers, we must do the same for the pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall:** Pooling layers compress the data outputted from the convolutional layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How:** Input is sectioned into small pieces. One element is chosen from each section and is fed into a smaller, output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will focus on **maxpooling**, which chooses the element with the highest value in each section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maxpooling visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:34:46.728603Z",
     "start_time": "2018-03-08T20:34:46.724063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"350\"\n",
       "            src=\"assets/cnns/maxpool.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd1901cb940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"assets/cnns/maxpool.html\", 600, 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So now to create the maxpool layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:41:21.227465Z",
     "start_time": "2018-03-08T20:41:21.154589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /darknet/src/image.h:9:0,\n",
      "                 from /darknet/src/maxpool_layer.h:4,\n",
      "                 from /tmp/tmpm7mfjvzj.c:2:\n",
      "/darknet/src/box.h:3:21: fatal error: darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include \"/darknet/include/darknet.h\"\n",
    "#include \"/darknet/src/maxpool_layer.h\"\n",
    "\n",
    "maxpool_layer make_maxpool_layer_ex(int batch, int h, int w, int c, int size, int stride, int padding)\n",
    "{\n",
    "    /*WRITE\n",
    "    \n",
    "    Just like in make_convolutional_layer, we have to build our maxpool layer.\n",
    "    */\n",
    "    //<><><><><><><><><><><><><>\n",
    "    maxpool_layer l = {0};\n",
    "    l.type = MAXPOOL;\n",
    "    l.batch = batch;\n",
    "    l.h = h;\n",
    "    l.w = w;\n",
    "    l.c = c;\n",
    "    l.inputs = h*w*c;\n",
    "    l.pad = padding;\n",
    "    \n",
    "    \n",
    "    l.out_w = (w + 2*padding)/stride;\n",
    "    l.out_h = (h + 2*padding)/stride;\n",
    "    l.out_c = c;\n",
    "    l.outputs = l.out_h * l.out_w * l.out_c;\n",
    "    int output_size = l.out_h * l.out_w * l.out_c * batch;\n",
    "    \n",
    "    l.size = size;\n",
    "    l.stride = stride;\n",
    "    \n",
    "    l.indexes = calloc(output_size, sizeof(int));\n",
    "    l.output =  calloc(output_size, sizeof(float));\n",
    "    l.delta =   calloc(output_size, sizeof(float));\n",
    "    l.forward = forward_maxpool_layer;\n",
    "    l.backward = backward_maxpool_layer;\n",
    "    //<><><><><><><><><><><><><><><><><><><><><><><>\n",
    "    #endif\n",
    "    fprintf(stderr, \"max          %d x %d / %d  %4d x%4d x%4d   ->  %4d x%4d x%4d\\n\", size, size, stride, w, h, c, l.out_w, l.out_h, l.out_c);\n",
    "    return l;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have explained the set up of the convolutional and pooling layers, give a shot at implementing the forward / backward passes of each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions integral to training our ConvNet:\n",
    "- forward_convolutional_layer() = Applies convolution onto a given convolutional layer\n",
    "\n",
    "- backward_convolutional_layer() = Applies backpropagation in a given convolutional layer, gets the rate of change of weights\n",
    "\n",
    "\n",
    "**things to keep in mind:**\n",
    " - net.workspace => allocated space for the network data. i.e. TELLS YOU WHICH LAYER YOU ARE CURRENTLY ON and ITS DATA\n",
    " - l.batch => # of samples fed into the convolutional layer per run through\n",
    " - gemm() = General Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:39:05.252695Z",
     "start_time": "2018-03-08T20:39:05.212875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /darknet/src/convolutional_layer.h:4:0,\n",
      "                 from /tmp/tmphtk6xi1k.c:2:\n",
      "/darknet/src/cuda.h:4:21: fatal error: darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include \"/darknet/include/darknet.h\"\n",
    "#include \"/darknet/src/convolutional_layer.h\"\n",
    "\n",
    "void main()\n",
    "{\n",
    "    convolutional_layer l = make_convolutional_layer(1, 5, 5, 3, 2, 1, 5, 2, 1, RELU, 1, 0, 0, 0);\n",
    "    l.batch_normalize = 1;\n",
    "    float data[] = {1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3};\n",
    "    net.input = data;\n",
    "    forward_convolutional_layer_student(l);\n",
    "    backward_convolutional_layer_student(l);\n",
    "}\n",
    "\n",
    "void forward_convolutional_layer_student(convolutional_layer l, network net)\n",
    "{\n",
    "    //<><><><><><><><><><><><<><><><><><><><><><><>\n",
    "    int i, j;\n",
    "\n",
    "    fill_cpu(l.outputs*l.batch, 0, l.output, 1);\n",
    "\n",
    "    if(l.xnor){\n",
    "        binarize_weights(l.weights, l.n, l.c/l.groups*l.size*l.size, l.binary_weights);\n",
    "        swap_binary(&l);\n",
    "        binarize_cpu(net.input, l.c*l.h*l.w*l.batch, l.binary_input);\n",
    "        net.input = l.binary_input;\n",
    "    }\n",
    "    //<><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "/////////////////////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    //<><><><><><><><><><><><><><><>\n",
    "    int m = l.n/l.groups;               ///\n",
    "    int k = l.size*l.size*l.c/l.groups; ///\n",
    "    int n = l.out_w*l.out_h;            ///output area size\n",
    "    //<><><><><><><><><><><><><><><><>\n",
    "\n",
    "    //<><><><><><><><><><><><><><><><> except: i < ?????, j < ?????\n",
    "    for(i = 0; i < l.batch; ++i){\n",
    "        for(j = 0; j < l.groups; ++j){   //in case of AlexNet implementation, we must account for the groups \n",
    "    //<><><<><><><><><><><><><><><><><><>\n",
    "            //configure the memory placement of the pointers\n",
    "            float *a = l.weights + j*l.nweights/l.groups;   // = ???  + l.???*j/l.?????          kernel\n",
    "            float *b = net.workspace;                       // = ???                             current layer\n",
    "            float *c = l.output + (i*l.groups + j)*n*m;     // = ??? + (i*l.groups + j)*m*?      outputs\n",
    "\n",
    "            im2col_cpu(net.input + (i*l.groups + j)*l.c/l.groups*l.h*l.w,  \n",
    "                l.c/l.groups, l.h, l.w, l.size, l.stride, l.pad, b);   /////GIVEN\n",
    "            gemm(0,0,m,n,k,1,a,k,b,n,1,c,n); // General Matrix multiplication ( TA, TB, M, N, K, ALPHA,A,lda, B, ldb,BETA,C,ldc);\n",
    "            //CAll this to test GEMM to show ppl void time_random_matrix(int TA, int TB, int m, int k, int n)\n",
    "        \n",
    "        }\n",
    "    }\n",
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    if(l.batch_normalize){\n",
    "        forward_batchnorm_layer(l, net);\n",
    "    } else {\n",
    "        add_bias(l.output, l.biases, l.batch, l.n, l.out_h*l.out_w);\n",
    "    }\n",
    "\n",
    "    activate_array(l.output, l.outputs*l.batch, l.activation);\n",
    "    if(l.binary || l.xnor) swap_binary(&l);\n",
    "}\n",
    "\n",
    "//Backpropagation on a convolutional layer\n",
    "void backward_convolutional_layer_student(convolutional_layer l, network net)\n",
    "{\n",
    "    \n",
    "    //<><><><><><><><><><>\n",
    "    int i, j;\n",
    "    int m = l.n/l.groups;\n",
    "    int n = l.size*l.size*l.c/l.groups;\n",
    "    int k = l.out_w*l.out_h;\n",
    "    //<><><><><><>\n",
    "    \n",
    "    gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta); //gets rate of change \n",
    "\n",
    "    if(l.batch_normalize){\n",
    "        backward_batchnorm_layer(l, net);\n",
    "    } else {\n",
    "        backward_bias(l.bias_updates, l.delta, l.batch, l.n, k);\n",
    "    }\n",
    "    \n",
    "    \n",
    "////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    //<><><><><><><><><><><>\n",
    "    for(i = 0; i < l.batch; ++i){\n",
    "        for(j = 0; j < l.groups; ++j){\n",
    "    //<><<><>><><><><><><><>\n",
    "            float *a = l.delta + (i*l.groups + j)*m*k;   //updates each image in batch, group for group.\n",
    "            float *b = net.workspace;\n",
    "            float *c = l.weight_updates + j*l.nweights/l.groups;\n",
    "\n",
    "            float *im = net.input+(i*l.groups + j)*l.c/l.groups*l.h*l.w;\n",
    "                \n",
    "            //GIVEN\n",
    "            im2col_cpu(im, l.c/l.groups, l.h, l.w, \n",
    "                    l.size, l.stride, l.pad, b);\n",
    "            \n",
    "            gemm(0,1,m,n,k,1,a,k,b,k,1,c,n);  //gemm(0,1,?,?,?,1,?,?,?,?,1,?,?)\n",
    "\n",
    "            if(net.delta){\n",
    "                a = l.weights + j*l.nweights/l.groups;\n",
    "                b = l.delta + (i*l.groups + j)*m*k;\n",
    "                c = net.workspace;\n",
    "\n",
    "                gemm(1,0,n,k,m,1,a,n,b,k,0,c,k); //apply the general matrix mult\n",
    "\n",
    "                col2im_cpu(net.workspace, l.c/l.groups, l.h, l.w, l.size, l.stride, \n",
    "                    l.pad, net.delta + (i*l.groups + j)*l.c/l.groups*l.h*l.w);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "//////////////////////////////////////////////////////////////////////////////////////////\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, maxpooling is much much simpler to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:45:14.468579Z",
     "start_time": "2018-03-08T20:45:14.440356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /darknet/src/image.h:9:0,\n",
      "                 from /darknet/src/maxpool_layer.h:4,\n",
      "                 from /tmp/tmpuc0k5fj0.c:2:\n",
      "/darknet/src/box.h:3:21: fatal error: darknet.h: No such file or directory\n",
      "compilation terminated.\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "#include \"/darknet/include/darknet.h\"\n",
    "#include \"/darknet/src/maxpool_layer.h\"\n",
    "\n",
    "void main()\n",
    "{\n",
    "    //make_maxpool_layer(int batch, int h, int w, int c, int size, int stride, int padding)\n",
    "    maxpool_layer l = make_maxpool_layer(1, 5, 5, 3, 5, 1, 1);\n",
    "    float data[] = {1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        1,1,1,1,1,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        2,2,2,2,2,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3,\n",
    "        3,3,3,3,3};\n",
    "    net.input = data;\n",
    "    forward_maxpool_layer_student(l, net);\n",
    "    backward_maxpool_layer_student(l, net);\n",
    "}\n",
    "\n",
    "\n",
    "void forward_maxpool_layer_student(const maxpool_layer l, network net)\n",
    "{\n",
    "    int b,i,j,k,m,n;\n",
    "    int w_offset = -l.pad;\n",
    "    int h_offset = -l.pad;\n",
    "\n",
    "    int h = l.out_h;\n",
    "    int w = l.out_w;\n",
    "    int c = l.c;\n",
    "////////////////////////////////////////////////////////////\n",
    "    \n",
    "    //<><><><><><<><><>\n",
    "    for(b = 0; b < l.batch; ++b){\n",
    "        for(k = 0; k < c; ++k){\n",
    "            for(i = 0; i < h; ++i){\n",
    "                for(j = 0; j < w; ++j){\n",
    "    //<><><><><><><><><>><><><<><>\n",
    "                    int out_index = j + w*(i + h*(k + c*b));\n",
    "                    \n",
    "                    //?????????????????????????/\n",
    "                    float max = -FLT_MAX;\n",
    "                    int max_i = -1;\n",
    "                    //??????????????????????????\n",
    "                    for(n = 0; n < l.size; ++n){\n",
    "                        for(m = 0; m < l.size; ++m){\n",
    "                            int cur_h = h_offset + i*l.stride + n;\n",
    "                            int cur_w = w_offset + j*l.stride + m;\n",
    "                            int index = cur_w + l.w*(cur_h + l.h*(k + b*l.c));\n",
    "                            int valid = (cur_h >= 0 && cur_h < l.h &&\n",
    "                                         cur_w >= 0 && cur_w < l.w);\n",
    "                            float val = (valid != 0) ? net.input[index] : -FLT_MAX;\n",
    "                            max_i = (val > max) ? index : max_i;\n",
    "                            max   = (val > max) ? val   : max;\n",
    "                        }\n",
    "                    }\n",
    "                    //\n",
    "                    \n",
    "                    l.output[out_index] = max;\n",
    "                    l.indexes[out_index] = max_i;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "////////////////////////////////////////////////////////////////\n",
    "}\n",
    "\n",
    "void backward_maxpool_layer(const maxpool_layer l, network net)\n",
    "{\n",
    "    int i;\n",
    "    int h = l.out_h;\n",
    "    int w = l.out_w;\n",
    "    int c = l.c;\n",
    "    for(i = 0; i < h*w*c*l.batch; ++i){ // i < ?????????\n",
    "        int index = l.indexes[i];\n",
    "        net.delta[index] += l.delta[i]; //??????????????????????/\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T20:44:07.240842Z",
     "start_time": "2018-03-07T20:44:07.234969Z"
    }
   },
   "source": [
    "# 4. Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished writing our functions, let's plug them into the source code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append code to source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In your folder viewer, go to [PATH TO THIS REPO]/meetings/darknet/src/\n",
    "2. Copy the inner code of each function and place them in the corresponding C file and function name\n",
    "    - E.g. In **convolutional_layer.c:** forward_convolutional_layer_student() **->** forward_convolutional_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open up your terminal (or Command Prompt) and **cd** to **[PATH TO THIS REPO]/meetings/darknet/**\n",
    "\n",
    "2. **make**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T21:10:43.189915Z",
     "start_time": "2018-03-08T21:10:43.165957Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmpjjb1cze6.c:2:1: error: expected identifier or '(' before '.' token\n",
      " ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg\n",
      " ^\n",
      "[C kernel] GCC exited with code 1, the executable will not be executed"
     ]
    }
   ],
   "source": [
    "//Paste this into your terminal/command prompt\n",
    "./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg\n",
    "\n",
    "\n",
    "//You can mess around with different pictures and the threshold level:\n",
    "./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg -thresh 0\n",
    "\n",
    "//And detect mutiple objects in an image:\n",
    "./darknet detect cfg/yolo.cfg yolo.weights data/horses.jpg\n",
    "\n",
    "\n",
    "//You can even detect and classify objects in real-time!\n",
    "//Follow the guide here to install cuda and OpenCV: https://pjreddie.com/darknet/install/#cuda\n",
    "// ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yay u did it"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "github": "thedibaccle",
    "name": "Richard DiBacco"
   }
  ],
  "hide_input": false,
  "kernelspec": {
   "display_name": "C",
   "language": "c",
   "name": "c"
  },
  "language_info": {
   "file_extension": ".c",
   "mimetype": "text/plain",
   "name": "c"
  },
  "livereveal": {
   "footer": "<footer id=\"slide_foot\">\n  <div  id=\"slide_foot-brand\">\n    <span class=\"ucfsigai-brand\"></span>\n  </div>\n  <div  id=\"slide_foot-unit\">\n    <span class=\"text-gold\"> U2: </span>&nbsp;<span class=\"text-white\"> Convolutional Neural Networks </span>\n  </div>\n  <a    id=\"slide_foot-attend\" href=\"https://goo.gl/\">\n      <span class=\"text-white\"> https://goo.gl/ </span>\n  </a>\n  <div  id=\"slide_foot-date\">\n    <span class=\"text-white\"> Feb 22, 2018 </span>\n  </div>\n</footer>\n",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
