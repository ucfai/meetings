{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some voo-doo Jupyter magicks\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do We Really Need English Lit Anymore? ;p\n",
    "\n",
    "## Mar 29, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be building a 3-layer LSTM (though it can be easily extended) using the [TensorFlow's RNN API](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/ops/rnn_cell_impl.py#L476)'s implementation.\n",
    "\n",
    "Ultimately, its important to understand how LSTMs work, but in practice, I've never seen it hand-coded except in \"deep learning\" libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About today's dataset, its all of William Shakespeare's gibberish (jk, he's supposedly real dope). Ultimately, we won't be training this network on it, but I'll have another notebook running in the background that will be performing the optimizations.\n",
    "> **NOTE:** It took about 30 minutes to run on my 1080 Ti at home. Unless you use a GPU, you'll be waiting a while &ndash; I don't advise doing anything with this unless you have a GPU, or a desktop you can do without for some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**What you should leave with:** You should leave with a practical understanding of how to utilize the TensorFlow RNN API, which gives us access to more than LSTMs and RNNs (as you'll see below). Ultimately, there's a lot of parameter fiddling we could implement to determine the best setup for our network (I'm using some of the parameter settings from [Andrej Karpathy's blog bost on RNNs](karpathy.github.io/2015/05/21/rnn-effectiveness/)). \n",
    "\n",
    "You should continue to foster that itch that was (hopefully) sparked in the first workshop &ndash; sadly we'll likely switch to a different library in the fall; but this workshop ought to further showcase how using these can speed up your building of models to test out ideas and hypotheses you may have.\n",
    "\n",
    "Some of the only times where this isn't the case is when you're building a new architecture, but, uhh... we're not that good, yet. :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "1. [Building Our Model](#1.-Building-Our-Model)\n",
    "1. [Writing up the Training Phase](#2.-Writing-Up-the-Training-Phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is essentially some initial setup, allowing us to write up parts of our `Model` as cells in Jupyter rather that a massive code-cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class initialization\n",
    "class Model:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing everything we need from TensorFlow to build the LSTM.\n",
    "\n",
    "```python\n",
    "from tensorflow.contrib import rnn # www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/rnn\n",
    "from tensorflow.contrib import legacy_seq2seq # www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/legacy_seq2seq\n",
    "```\n",
    "The actual links: \n",
    "- [`from tensorflow.contrib import rnn`](www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/rnn) This is the RNN library which basically houses everything we could ever want for the lecture.\n",
    "- [`from tensorflow.contrib import legacy_seq2seq`](www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/legacy_seq2seq) This is deprecated, not sure when it'll be taken out &ndash; but for the purposes of teaching, it works. We'll be using it to build a [Sequence to Sequence](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) model.\n",
    "\n",
    "As noted in the blog post (the Seq2Seq link), these models are typically used in machine translation, but they can also be used in text generation. We'll, obviously, be partaking in the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq as ls2s\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is how Python implements `switch` statements :/ we\n",
    "##   have to use a dictionary instead, but we'll be fine. ^^\n",
    "models = {\n",
    "    \"rnn\" : rnn.BasicRNNCell,\n",
    "    \"gru\" : rnn.GRUCell,\n",
    "    \"lstm\": rnn.BasicLSTMCell,\n",
    "    \"nas\" : rnn.NASCell,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(args, training):\n",
    "    ## we want a batch_size and seq_length of 1 for sampling\n",
    "    args.batch_size = args.batch_size if training else 1\n",
    "    args.seq_length = args.seq_length if training else 1\n",
    "    return args\n",
    "\n",
    "def build_rnn(args):\n",
    "    ## grabbing the desired model, if it exists\n",
    "    assert args.model in models.keys(), \n",
    "        \"-> model type unsupported: {}\".format(args.model)\n",
    "    cell_fn = models[args.model]\n",
    "    \n",
    "    ## building the LSTM cells\n",
    "    cells = [cell_fn(args.rnn_size) for _ in range(args.n_layers)]\n",
    "    \n",
    "    ## wrapping the LSTM cells into LSTM network we desire\n",
    "    ##  - this is akin to tacking on additional hidden layers to \n",
    "    ##    a ANN\n",
    "    return rnn.MultiRNNCell(cells)\n",
    "    \n",
    "def build_reshape_input(args, inp):\n",
    "    embeds = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "    inputs = tf.nn.embedding_lookup(embeds, inp)\n",
    "    \n",
    "    ## split the input into chunks of `seq_length`\n",
    "    inputs = tf.split(inputs, args.seq_length, 1)\n",
    "    ## trim the split to remove extraneous single value dimensions\n",
    "    ## - ex: arry.shape = [1, 3, 6, 1, 1, 2] -> [3, 6, 2], b/c the values of \n",
    "    ##   [1] just means unnecessary indexing (getting )\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "    \n",
    "    return embeds, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, args, training=True):\n",
    "    self.args = training(args, training)\n",
    "    \n",
    "    self.cell = build_rnn(args)\n",
    "    \n",
    "    self.input   = tf.placeholder(tf.int32, \n",
    "                                  [args.batch_size, args.seq_length])\n",
    "    self.targets = tf.placeholder(tf.int32, \n",
    "                                  [args.batch_size, args.seq_length])\n",
    "    ## intiial state - this should be 0s b/c we can't possibly have\n",
    "    ##   recall when we just start learning\n",
    "    self.init_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "    \n",
    "    ## rnn language model, using softmax as the activation function\n",
    "    with tf.variable_scope(\"rnnlm\"):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", \n",
    "                                    [args.rnn_size, args.vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", \n",
    "                                    [args.vocab_size])\n",
    "\n",
    "    embeds, inputs = build_reshape_input(args, self.input)    \n",
    "\n",
    "    ## as the docs spec: https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/contrib/legacy_seq2seq/rnn_decoder#args\n",
    "    ##   we won't be using the index, so in standard python we spec \"don't care\" by '_'\n",
    "    def loop(prev, _):\n",
    "        ## standard neural net math :joy:\n",
    "        prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "        ## \n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embeds, prev_symbol)\n",
    "\n",
    "    \n",
    "    outputs, last_state = ls2s.rnn_decoder(inputs, self.init_state, cell, \n",
    "                                           loop_function=loop if not training else None, \n",
    "                                           scope=\"rnnlm\")\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])\n",
    "\n",
    "    self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    self.probs  = tf.nn.softmax(self.logits)\n",
    "    loss = ls2s.sequence_loss_by_example(\n",
    "            [self.logits],\n",
    "            [tf.reshape(self.targets, [-1])],\n",
    "            [tf.ones([args.batch_size * args.seq_length])])\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        self.cost = tf.reduce_sum(loss) / (args.batch_size * args.seq_length)\n",
    "        \n",
    "    self.final_state = last_state\n",
    "    \n",
    "    self.lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), args.grad_clip)\n",
    "    \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        \n",
    "    self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    tf.summary.histogram(\"logits\", self.logits)\n",
    "    tf.summary.histogram(\"loss\", loss)\n",
    "    tf.summary.scalar(\"train_loss\", self.cost)\n",
    "    \n",
    "Model.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, sess, chars, covab, num=200, prime=\"The \", sampling_type=1):\n",
    "    state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "    \n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeroes((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {self.input_data: x, self.initial_state: state}\n",
    "        [state] = sess.run([self.final_state], feed)\n",
    "        \n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    \n",
    "    ret = prime; char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeroes((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {self.input_data: x, self.initial_state: state}\n",
    "        [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "        p = probs[0]\n",
    "        \n",
    "        sample_argmax = bool(sampling_type == 0 or (sampling_type == 2 and char != ' '))\n",
    "        sample = np.argmax(p) if sample_argmax else weighted_pick(p)\n",
    "        \n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "        \n",
    "    return ret\n",
    "\n",
    "Model.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Args\n",
    "\n",
    "args = {\n",
    "    \"data_dir\": \"/data/shakespeare/\",\n",
    "    \"rnn_size\": 700,\n",
    "    \"n_layers\":   3,\n",
    "}\n",
    "\n",
    "args = Args(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrej (he's dope, btw) have a few data sources we can use. You'll find them here: https://cs.stanford.edu/people/karpathy/char-rnn/. However, tonight we'll be using the Shakespearian text &ndash; the initial plans were to use the Linux Kernel, but that came out to ~750MB of text and would have taken 11.5 days to train.\n",
    "\n",
    "I'll train it over the beginning of the summer and provide the weights file if anyone wants it, though. :D\n",
    "\n",
    "To snag the Shakespeare corpus, let's run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt > /data/shakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid giving everyone a lesson on preprocessing, that's been abstracted away &ndash; you can take a gander at the `utils.py` file in this semester's directory for an inside on how we get it into a usable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TextLoader\n",
    "\n",
    "data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "args.vocab_size = data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since none of us have bomb GPUs in our laptops, we'll load the checkpoint file that has all the weights and such from when I trained this network. I also have one training right now, as you saw at the beginning, and we might wanna compare the outputs, since randomness does play a minor role and with even a 2 hour difference, we can get slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "chkpt = tf.train.get_checkpoint_state(args.data_dir)\n",
    "assert chkpt, \"No checkpoint found.\"\n",
    "assert chkpt.model_checkpoint_path, \"No model path found in `chkpt`.\"\n",
    "\n",
    "## Loading the prior configuration, in terms of everything listed in `utils.Args`\n",
    "##   we can take a gander at the parameters if you'd like.\n",
    "with open(args.data_dir + \"config.pkl\", \"rb\") as f:\n",
    "    saved_model_args = cPickle.load(f)\n",
    "\n",
    "## we have to have a vocabulary because this is how the network is penalized\n",
    "with open(args.data_dir + \"chars_vocab.pkl\", \"rb\") as f:\n",
    "    saved_chars, saved_vocab = cPickle.load(f)\n",
    "    \n",
    "saved_model_args.data_dir = args.data_dir\n",
    "saved_model_args.n_layers = saved_model_args.num_layers\n",
    "\n",
    "## building the model from previous parameters\n",
    "model = Model(saved_model_args, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last time, because TensorFlow is weird, we have to launch `tf.InteractiveSession()`, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing all the variables we've declared, as per TensorFlow's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit is only pertinent on the other notebook I'm running, since this is opening the TensorBoard and we can look at the network's progress there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "summaries = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(args.logs_dir + time.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully restoring what we had from the previous checkpoint..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.restore(sess, chkpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pretty_print, final_checkpoint\n",
    "strlen = {\n",
    "    \"batch\": str(len(str(args.num_epochs * data_loader.num_batches))),\n",
    "    \"epoch\": str(len(str(args.num_epochs))),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Network\n",
    "\n",
    "Sike! Okay, that was cruel; but honestly, let's skip training the network. Your laptop begs of you, don't be a cruel human. We'll go to the next section to generate some new Shakespeare, and if you ever sit in on an English Lit class again, you might just see this. ;p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(args.num_epochs):\n",
    "    ## decay the learning rate\n",
    "    upd_lr = args.lr * (args.decay ** e)\n",
    "    sess.run(tf.assign(model.lr, upd_lr))\n",
    "\n",
    "    ## reset batches\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    ## reset s_0\n",
    "    state = sess.run(model.initial_state)\n",
    "\n",
    "    for b in range(data_loader.num_batches):\n",
    "        st = time.time()\n",
    "        offset = e * data_loader.num_batches + b\n",
    "\n",
    "        x, y = data_loader.next_batch()\n",
    "        feed = {model.input_data: x, model.targets: y}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed[c] = state[i].c\n",
    "            feed[h] = state[i].h\n",
    "\n",
    "        run_args = [summaries, model.cost, mode.final_state, model.train_op]\n",
    "        summ, train_loss, state, _ = sess.run(run_args, feed)\n",
    "\n",
    "        writer.add_summary(summ, offset)\n",
    "\n",
    "        pretty_print(strlen, e, b, train_loss, st, end=time.time())\n",
    "        final_chkpt(sess, saver, save_dir, offset, \n",
    "                    args.num_epochs, \n",
    "                    data_loader.num_batches, \n",
    "                    args.save_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Replacing William Shakespeare, Featuring the Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = Model(args, training=False)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(args.data_dir)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    ## int(chars_to_sample) -> number of characters to sample\n",
    "    ## str(primer) -> the first bit of text to \"nudge\" the network\n",
    "    ## int(how_sample) -> 0 uses max() at each timestep, 1 samples on every timestep, 2 samples on every space\n",
    "    chars_to_sample = 500\n",
    "    primer = u' '\n",
    "    how_sample = 1\n",
    "    print(model.sample(sess, chars, vocab, chars_to_sample, primer, how_sample).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " HENRY VI:\n",
    "Far be the thought of this frost; he cannot play me.\n",
    "\n",
    "ABHORSON:\n",
    "Bate, you from me.\n",
    "\n",
    "ESCALUS:\n",
    "Well, I do so.\n",
    "\n",
    "POMPEY:\n",
    "Women are yourself, sir? a dell the heaven, and I\n",
    "will push upon thee: long it is to go to it.\n",
    "Thou wouldst have leave to live, I look on thee.\n",
    "\n",
    "All:\n",
    "Happily make thee speak: I say upon thee.\n",
    "\n",
    "CAPULET:\n",
    "And learn to marry, my lord, to charge thee.\n",
    "\n",
    "SEBASTIAN:\n",
    "What answer place?\n",
    "\n",
    "MENENIUS:\n",
    "Only he may profane; let us be content:\n",
    "How could I might have an unwilling love!\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "penish to you,\n",
    "And catch attorn your grace to do so.\n",
    "\n",
    "MERCUTIO:\n",
    "You will not, then?\n",
    "\n",
    "First Musician:\n",
    "No.\n",
    "\n",
    "PETER:\n",
    "O, I still have none;\n",
    "You would be deceived; your quoding throne\n",
    "While she's fallen with victors' son, with one\n",
    "And make myself have attendantly.\n",
    "I never look'd for better attendantnron.\n",
    "Speak not what, within, rouse all, as you say,\n",
    "To see if all these still--beggare, dull-brait,\n",
    "'Thus I took me to a figure of us by leave.\n",
    "When I have seen consequed by no greater,\n",
    "Seldom with over-blown,"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "github": "ionlights",
    "name": "John Muchovej"
   }
  ],
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "footer": "<footer id=\"slide_foot\">\n  <div  id=\"slide_foot-brand\">\n    <span class=\"ucfsigai-brand\"></span>\n  </div>\n  <div  id=\"slide_foot-unit\">\n    <span class=\"text-gold\"> U3: </span>&nbsp;<span class=\"text-white\"> Recurrent Neural Networks </span>\n  </div>\n  <a    id=\"slide_foot-attend\" href=\"https://goo.gl/\">\n      <span class=\"text-white\"> https://goo.gl/ </span>\n  </a>\n  <div  id=\"slide_foot-date\">\n    <span class=\"text-white\"> Mar 15, 2018 </span>\n  </div>\n</footer>\n",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
