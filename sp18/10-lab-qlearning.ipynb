{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcoming Car Troubles with Q-Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<p>So, we gave two lectures in preparation for this workshop. You can find them on GitHub and Google Slides.</p>\n",
    "<ol>\n",
    "    <li>Can Machines Learn from Experience? (RL P1) [GitHub][rlp1-gh], [Google Slides][rlp1-gs]</li>\n",
    "    <li>Can Machines Learn from Experience? (RL P2) [GitHub][rlp2-gh], [Google Slides][rlp2-gs]</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "[rlp1-gh]: https://github.com/ucfsigai/meetings/blob/master/sp18/07-lec-reinforcement-learning-p1.pdf\n",
    "[rlp1-gs]: https://docs.google.com/presentation/d/1wHPmkonCFCsLBIBLZDYvxHs6EeW6vLDsXCxNOq0eNlY\n",
    "[rlp2-gh]: https://github.com/ucfsigai/meetings/blob/master/sp18/08-lec-reinforcement-learning-p2.pdf\n",
    "[rlp2-gs]: https://docs.google.com/presentation/d/1x1Mi_bhlGmNKQ7brraWJUMG7B9Y_WqU9970ni-ic5FY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDPs are, informally, a set of states which are contected together in some way. Each of those connections have some probability of being the next action you take, based on your current state. **e.g.** Imagine you're at an intersection, there's some probability that you'll proceed straight ahead, move to the left, move to the right, move backward, or do nothing (hopefully nothing more :p). That's a single step in a MDP.\n",
    "\n",
    "You can find a [visual here][mdp-d3]; make sure, if you modify the transition matrix, such that each row sums to 1.\n",
    "\n",
    "More formally... MDPs are a \"5-tuple\":\n",
    "\n",
    "- $S$, a **_finite_** set of states\n",
    "- $A$, a **_finite_** set of actions\n",
    "- $P_a(s, s') = Pr(s_{t+1} = s' | s_t = s, a_t = a)$, the probability that an action $a$, in state $s$, at time $t$ will lead to state $s'$ in time $t + 1$\n",
    "- $R_a(s, s')$, is the immediate reward received after transitioning from $s \\rightarrow s'$, due to action $a$\n",
    "- $\\gamma \\in [0, 1]$ is the discount factor, which affects an agent's emphasis on future rewards.\n",
    "\n",
    "[mdp-d3]: http://localhost:19972/notebooks/sp18/assets/markov/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make these choices based on a policy ($\\pi$), which maps states to actions. More specifically, we map the **_current state_** to some action. We can do this because of the Markov Property.\n",
    "$$ \\pi(s): S \\rightarrow A $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Policy and Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [RL P2][rlp2-gs], we talked about Policy and Value Iteration. Some of the key snippets you need to know include...\n",
    "\n",
    "Policy and Value Iteration (PVI) are considered model-based learning algorithms. Model-based learning means that the agent knows the MDP used to model the environment, before it begins exploring. This allows the agent to plan its actions within the given environment before taking any actions (this is often referring to as offline learning).\n",
    "\n",
    "\n",
    "[rlp2-gs]: https://docs.google.com/presentation/d/1x1Mi_bhlGmNKQ7brraWJUMG7B9Y_WqU9970ni-ic5FY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slight Trangent: Value Function $\\rightarrow V(s)$\n",
    "\n",
    "Simply, the Value Function (typically denoted by $V(s)$), is a measure of \"how good is this state?\" The formulation of $V(s)$ is...\n",
    "\n",
    "$$ V^\\pi(s) = E[\\Sigma_{i=1}^T \\gamma^{i-1} \\cdot r_i] \\quad \\forall s \\in S $$\n",
    "\n",
    "But... like with most things, there's an _Optimal Value Function_, which presents the best value among its peers.\n",
    "\n",
    "$$ V^*(s) = max_\\pi V^\\pi(s) \\quad \\forall s \\in S $$\n",
    "\n",
    "Knowing this, we can also determine the _Optimal Policy_! :D\n",
    "\n",
    "$$ \\pi^* = arg max_\\pi V^\\pi(s) \\quad \\forall s \\in S $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Tangets: The Q-Function $\\rightarrow Q(s)$\n",
    "\n",
    "The Q-Function (also in RL P2), is a measure of \"how good is it for me to pick action $a$ while in state $s$?\" This differs from the Value Function $V(s)$ mostly in that $Q(s, a)$ considers the state **_and_** possible action(s), while $V(s)$ simply considers the current state.\n",
    "\n",
    "Due to their similarity, we can actually restate our _Optimal Value Function_ as:\n",
    "\n",
    "$$ V^*(s) = max_a Q^*(s, a) \\quad \\forall s \\in S $$\n",
    "\n",
    "So, if we know $Q^*(s, a)$ (the optimal Q-Function), we can not only extract the Optimal Value Function, but we can also derive the Optimal Policy! XD\n",
    "\n",
    "$$ \\pi^* = arg max_a Q^*(s, a) \\quad \\forall s \\in S $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The fabled (or soon to be) Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [equations][bellman-eqn] are based on a variety of discoveries by [Richard Bellman][rb-wiki] in Optimization. Essentially, they model the idea that...\n",
    "> The value of a decision is based on the payoff of some initial choices and the value of the remaining decisions that result from those initial choices.\n",
    "\n",
    "**Tangent:** This is pertinent to Dynamic Programming, which is all about optimization through breaking down a single problem into various subproblems.\n",
    "\n",
    "[rb-wiki]: https://en.wikipedia.org/wiki/Richard_E._Bellman\n",
    "[bellman-eqn]: https://en.wikipedia.org/wiki/Bellman_equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Q^*(s, a) &= R(s, a) + \\gamma E_{s'}[V^*(s')] \\\\\n",
    "Q^*(s, a) &= R(s, a) + \\gamma \\Sigma_{s' \\in S}p(s'|s, a)V^*(s')\n",
    "\\end{align}\n",
    "\n",
    "We know, from earlier, though... \n",
    "\n",
    "\\begin{align}\n",
    "V^*(s) &= max_a Q^*(s, a) \\\\\n",
    "V^*(s) &= max_a [R(s, a) + \\gamma \\Sigma_{s' \\in S}p(s'|s, a)V^*(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, PVI (Policy and Value Iteration) rely on these equations to compute $V^*(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration (VI)\n",
    "\n",
    "Simply, VI computes the optimal state-value by iteratively updating $Q(s, a)$ and $V(s)$ until they converge. A perk of VI is that its guaranteed t oconverge to an optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration (PI)\n",
    "\n",
    "Something worth noting, though, is that there are cases where the policy converges before the value function does.  Also note that the agent only cares about finding the optimal policy. $V(s)$ must converge, as with VI; but improves the value function based on updates to the policy. PI, like VI, is guaranteed to converge, but in some cases may take require fewer iterations to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Value and Policy Iteration\n",
    "\n",
    "- They're both used for offline planning (offline learning).\n",
    "- Policy Iteration is computationally efficient on the path to convergence, but each iteration is more computationally expensive than Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model-free learning algorithm, which relies on this equation:\n",
    "\n",
    "\\begin{align}\n",
    "Q(s, a) &= (1 - \\alpha) \\cdot Q(s, a) + \\alpha Q_{obs}(s, a) \\\\\n",
    "Q_{obs}(s, a) &= r(s, a) + \\gamma \\cdot max_{a'} Q(s', a')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## We Qan almost learn. Just need some setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using a toolkit called [Gym][site:openai/gym] [(on GitHub)][openai/gym], which is built by [OpenAI][site:openai]. Gym is an attempt at creating semi-standardized environments in which agents can be tested and have their results compared more effectively.\n",
    "\n",
    "If you're interested in learning more about Gym, check out their [documentation][site:openai/gym/docs], it's a bit sparse; but should be enough to get you started, at least with using their prefab [environments][site:openai/gym/docs#env].\n",
    "\n",
    "[site:openai]: https://openai.com/\n",
    "[site:openai/gym]: https://gym.openai.com\n",
    "[openai/gym]: https://github.com/openai/gym\n",
    "[site:openai/gym/docs]: https://gym.openai.com/docs/\n",
    "[site:openai/gym/docs#env]: https://gym.openai.com/docs/#environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Mountain Car](gym/mountaincar)\n",
    "\n",
    "<video width=\"500\" height=\"250\">\n",
    "    <source src=\"https://gym.openai.com/v2018-02-21/videos/MountainCar-v0-270f34b9-f23e-4d95-a933-4c902b4f4435/original.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"Action Space: `{}`\".format(env.action_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"Observation Space: `{}`\".format(env.observation_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"`env_lo` = `{}`\".format(env.observation_space.low)))\n",
    "env_lo = env.observation_space.low\n",
    "display(Markdown(\"`env_hi` = `{}`\".format(env.observation_space.high)))\n",
    "env_hi = env.observation_space.high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounting factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ini = 1.0\n",
    "lr_min = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of not exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic and temporal limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "max_time = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of states..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the exploration we did earlier, we know that our `env.action_space` is a [`Discrete`][openai/gym/discrete] space; based on the definition of the [`Discrete`][openai/gym/discrete.n] space... we can use that to define our `n_actions` (number of actions).\n",
    "\n",
    "[openai/gym/discrete]: https://github.com/openai/gym/blob/master/gym/spaces/discrete.py\n",
    "[openai/gym/discrete.n]: https://github.com/openai/gym/blob/master/gym/spaces/discrete.py#L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dx = (env_hi - env_lo) / n_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to Q-Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observations2states(env, obs):\n",
    "    a = int((obs[0] - env_lo[0]) / env_dx[0])\n",
    "    b = int((obs[1] - env_lo[1]) / env_dx[1])\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the Q table\n",
    "q_table = np.zeros((n_states, n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(max_iter):\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    ## learning rate is decreased at each step\n",
    "    lr = max(lr_min, lr_ini * (0.85 ** (episode // 100)))\n",
    "\n",
    "    for _ in range(max_time):\n",
    "        ## make an attempt, and retribe an observation\n",
    "        a, b = observations2states(env, obs)\n",
    "\n",
    "        logits     = q_table[a][b]\n",
    "        logits_exp = np.exp(logits)\n",
    "\n",
    "        weighted_probs = logits_exp / np.sum(logits_exp)\n",
    "\n",
    "        explore = np.random.uniform(0, 1) < epsilon\n",
    "        distrib = None if explore else weighted_probs\n",
    "        action  = np.random.choice(env.action_space.n, p=distrib)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        ## we move into the \"next timestep\", so what is \"prev_action\" is \n",
    "        ## the Q value of the action taken above\n",
    "        a_, b_ = observations2states(env, obs)\n",
    "\n",
    "        ## prior action we took\n",
    "        prev_action = q_table[a][b][action]\n",
    "        ## action we might take before of prior action\n",
    "        next_action = q_table[a_][b_]\n",
    "\n",
    "        ## Q(s, a) = (1 - \\alpha) * Q(s, a) \n",
    "        ##           + \\alpha * [r(s, a) + \\gamma * max{Q(s', a')}]\n",
    "        ## some voodoo later...\n",
    "        ## Q(s, a) = Q(s, a) + \\alpha * [r(s, a) + \\gamma * max{Q(s', q')} - Q(s, a)]\n",
    "        q_table[a][b][action] = prev_action + lr * (reward + gamma * np.max(next_action) - prev_action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0 or episode == max_iter - 1:\n",
    "        print('iter: {0:5d} | reward: {1:5.5f}'.format(episode, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy=None, render=False):\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    ## note: `step_idx` for the discounting factor is initialized here\n",
    "    for step_idx in range(max_time):\n",
    "        \n",
    "        if render:\n",
    "            plt.imshow(env.render(mode='rgb_array'))\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        a, b = observations2states(env, obs)\n",
    "        action = env.action_space.sample() if policy is None else policy[a][b]\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += (gamma ** step_idx) * reward\n",
    "        \n",
    "        if done:\n",
    "            return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_policy = np.argmax(q_table, axis=2)\n",
    "np.set_printoptions(threshold=np.inf, linewidth=120)\n",
    "print(solution_policy)\n",
    "\n",
    "## Score the solutions\n",
    "solution_policy_scores = [run_episode(env, policy=solution_policy) for _ in range(100)]\n",
    "\n",
    "print(\"mean(reward): {0:5.5f}\".format(np.mean(solution_policy_scores)))\n",
    "\n",
    "## Visualize actions based on the best solution\n",
    "run_episode(env, policy=solution_policy, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
